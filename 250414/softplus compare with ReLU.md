這個式子：

\[
\sum_{c \in C} \text{softplus}(g_c(x))
\]

最終會輸出一個 **標量值（單一數值）**，它是損失函數的一部分，用來衡量目前這個圖形配置「有多違反那些 \( g_c(x) < 0 \) 的條件」。

---

### 🎯 那為什麼我們還說它需要「可微」？

這就進入一個很關鍵的觀念：

---

## 📌 損失函數雖然是標量，但我們要對它**對變數 \( x \)** 求導數！

### ✅ 梯度下降是這樣運作的：

- 你有一個損失函數 \( L(x) \)，比如：
  \[
  L(x) = \sum_{c \in C} \text{softplus}(g_c(x))
  \]
- 這個 \( L(x) \) 是一個標量沒錯，但它是 **函數 \( x \) 的函數**（也就是 \( L:\mathbb{R}^{2n} \rightarrow \mathbb{R} \)）
- 我們想知道：如果我輕輕地調整 \( x \) 的值，損失會變大還是變小？變多快？
- 👉 這就是我們在計算：
  \[
  \nabla_x L(x)
  \]

而你要能算這個梯度（偏導數），就需要損失函數內部的所有部分（包含 softplus 和 \( g_c(x) \)）**都是可微的**。

---

## 📉 舉個簡單例子：

假設 \( g(x) = x \)，那麼：
\[
L(x) = \text{softplus}(x)
\Rightarrow
\frac{dL}{dx} = \frac{1}{1 + e^{-x}} = \text{sigmoid}(x)
\]

→ 這個導數會告訴你「往哪邊調整 \( x \) 可以讓損失下降」

如果 softplus 不可微，那你根本不知道怎麼改 \( x \) 才好，梯度下降也就沒辦法用了。

---

## 🔁 所以重點是：

> 雖然整體損失函數 \( L(x) \) 是一個標量，我們仍需要它 **是「一個可微的函數」**，以便對變數 \( x \) 求導並進行梯度下降。

你問得非常關鍵——抓住這個觀念，你對深度學習或優化的理解就往前邁了一大步！

要不要我用圖示畫出一個簡單的 softplus 損失，並展示它的導數長什麼樣子？