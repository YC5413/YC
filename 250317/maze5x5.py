# -*- coding: utf-8 -*-
"""「Untitled1.ipynb」的副本

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WGPz3Bhyh4BQYZInpTTKz1uILhOoSwMa
"""

import numpy as np
import random

# 定義網格環境大小與目標點
grid_size = 5
target = (grid_size - 1, grid_size - 1)

# 定義動作與對應效果：0: 上, 1: 下, 2: 左, 3: 右
actions = [0, 1, 2, 3]
action_effects = {
    0: (-1, 0),  # 上
    1: (1, 0),   # 下
    2: (0, -1),  # 左
    3: (0, 1)    # 右
}
action_names = {0: "上", 1: "下", 2: "左", 3: "右"}

# Q-learning 參數
alpha = 0.1      # 學習率
gamma = 0.9      # 折扣因子
epsilon = 0.2    # 探索機率
episodes = 500   # 訓練回合數
max_steps = 100  # 每回合最大步數

# 初始化 Q 表，每個狀態 (x, y) 皆有四個動作的 Q 值
Q = {}
for i in range(grid_size):
    for j in range(grid_size):
        Q[(i, j)] = np.zeros(len(actions))
        print (Q[(i,j)])

print (Q[1,2])

def choose_action(state):
    """依據 ε-greedy 策略選擇動作"""
    if random.uniform(0, 1) < epsilon:
        return random.choice(actions)
    else:
        return int(np.argmax(Q[state]))

def take_action(state, action):
    """根據動作取得下一個狀態，若超出邊界則保持原位"""
    effect = action_effects[action]
    new_state = (state[0] + effect[0], state[1] + effect[1])
    if new_state[0] < 0 or new_state[0] >= grid_size or new_state[1] < 0 or new_state[1] >= grid_size:
        new_state = state
    return new_state

def get_reward(state):
    """依據是否達到目標給予獎勵或懲罰"""
    if state == target:
        return 100
    else:
        return -1

def best_path_from_start():
    """利用當前 Q 表從起點推導出最佳路徑 (避免無限迴圈)"""
    state = (0, 0)
    path = [state]
    steps = 0
    while state != target and steps < 50:
        action = int(np.argmax(Q[state]))
        next_state = take_action(state, action)
        # 如果下一步沒有變化或進入循環，則中斷
        if next_state == state or next_state in path:
            break
        path.append(next_state)
        state = next_state
        steps += 1
    return path

# Q-learning 訓練階段
for episode in range(episodes):
    state = (0, 0)  # 從起點開始
    for step in range(max_steps):
        action = choose_action(state)
        next_state = take_action(state, action)
        reward = get_reward(next_state)

        # 更新 Q 值：Q(s, a) <- Q(s, a) + α [ reward + γ max_a' Q(s', a') - Q(s, a) ]
        best_next = np.max(Q[next_state])
        Q[state][action] += alpha * (reward + gamma * best_next - Q[state][action])

        #print(f"Action:{action_names[action]},State:{state}, Next State:{next_state}, Reward: {reward}, Best Next: {best_next}, Q-value: {Q[state][action]}")
        state = next_state
        if state == target:
            break
        state = next_state
        if state == target:
            break

# 模擬測試，逐步顯示每一步的狀態與 Q 值變化
print("==== 模擬測試：從起點 (0,0) 到目標 (4,4) ====")
state = (0, 0)
step = 0
print("初始狀態:", state)
while state != target and step < 50:
    action = int(np.argmax(Q[state]))  # 採取最佳動作
    next_state = take_action(state, action)
    reward = get_reward(next_state)

    # 列印當前步驟資訊
    print(f"\n【Step {step+1}】")
    print("當前狀態:", state)
    print("選擇動作:", action, f"({action_names[action]})")
    print("下一個狀態:", next_state)
    print("獎勵:", reward)
    print("狀態", state, "的 Q 值:", Q[state])

    # 利用目前 Q 表推導最佳路徑
    current_best_path = best_path_from_start()
    print("從起點推導的最佳路徑:", current_best_path)

    state = next_state
    step += 1

if state == target:
    print("\n機器人成功到達目標!")
else:
    print("\n模擬步數超過限制，可能尚未收斂到正確路徑。")

